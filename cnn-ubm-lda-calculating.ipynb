{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os, re\nimport numpy as np\nimport soundfile as sf\nfrom IPython.display import clear_output\nimport pickle\nimport librosa\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import Model\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.metrics import det_curve, DetCurveDisplay\nimport plotly.express as px\nimport plotly\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go","metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Przygotowanie danych do modelu UBM, jego trening oraz liczenie LDA.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/all-people-df/df')\n\n# Ścieżka do folderu, w którym znajdują się katalogi z nagraniami osób.\nfile_path = '/kaggle/input/audio-to-train-model/train-clean-100'\n\n# Wyciągam wszystkie nazwy podfolderów z powyższej ścieżki (są to ID nagranych osób).\nsubfolders = [f.name for f in os.scandir(file_path) if f.is_dir()]\n\n# Sortuję ID nagranych osób (najpierw muszę zamienić ID na liczbę).\nsubfolders = sorted([int(item) for item in subfolders])\nsubfolders = np.array(subfolders)\n\ndf = df.loc[np.isin(np.array(df['ID']), subfolders)]\n\n# Tworzę oddzielne ramki dla kobiet i mężczyzn.\ndf_woman = df[df['SEX'] == ' F '].reset_index(drop=True)\ndf_man = df[df['SEX'] == ' M '].reset_index(drop=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folders_path = '/kaggle/input/audio-to-train-model/train-clean-100'\n\n# Wyciągam wszystkie podfoldery (ID osób) z głównego folderu z nagraniami.\nsubfolders = [f.name for f in os.scandir(folders_path) if f.is_dir()]\n# Sortuję ID osób i zamieniam je z powrotem na stringi (ID muszą być w formie tekstowej).\nsubfolders = sorted([int(item) for item in subfolders])\nsubfolders = [str(item) for item in subfolders]\n\n# Tworzę pełne ścieżki do folderów dla każdego ID.\npaths_with_ID = [folders_path + '/' + subfolder for subfolder in subfolders]\n\n# Tworzę dwie ramki danych do przechowywania ID i sumarycznej długości nagrań dla kobiet i mężczyzn.\ndata_frame_for_duration_woman = pd.DataFrame(columns=['ID', 'duration'])\ndata_frame_for_duration_man = pd.DataFrame(columns=['ID', 'duration'])\n\n# Pętla przez wszystkie osoby, aby obliczyć sumaryczną długość nagrań.\nfor path_with_ID in paths_with_ID:\n\n    # Zbieram ścieżki do folderów wewnątrz folderu danej osoby (podfoldery).\n    paths_inside_ID = [f.name for f in os.scandir(path_with_ID) if f.is_dir()]\n\n    # Tworzę pełne ścieżki do plików nagrań (plików .flac) dla każdego folderu wewnątrz ID.\n    full_paths_to_files = [path_with_ID + '/' + path_inside_ID for path_inside_ID in paths_inside_ID]\n\n    # Zbieram wszystkie pliki audio dla danej osoby.\n    all_files_for_ID = []\n    for full_path_to_files in full_paths_to_files:\n        files = [f.name for f in os.scandir(full_path_to_files) if f.is_file() and f.name.endswith('.flac')]\n        files = [full_path_to_files + '/' + file for file in files]\n        all_files_for_ID = all_files_for_ID + files\n\n    # Obliczam łączną długość nagrań danej osoby.\n    duration_in_seconds = 0\n    for file_for_ID in all_files_for_ID:\n        # Otwieram plik audio za pomocą SoundFile i obliczam długość nagrania na podstawie liczby próbek i częstotliwości próbkowania.\n        with sf.SoundFile(file_for_ID) as f:\n            frames = len(f)  # Liczba próbek (frames)\n            sample_rate = f.samplerate  # Częstotliwość próbkowania\n        duration = frames / sample_rate  # Długość nagrania w sekundach\n        duration_in_seconds = duration_in_seconds + duration  # Sumowanie długości wszystkich nagrań\n\n    # Wyciągam ID osoby z pełnej ścieżki.\n    ID = path_with_ID.split('/')[-1]\n    # Tworzę nowy rekord z ID i sumaryczną długością nagrań.\n    new_record = [ID, duration_in_seconds]\n    \n    # Sprawdzam, czy ID osoby należy do kobiet i dodaję dane do odpowiedniej ramki danych.\n    if np.isin(ID, df_woman['ID']):\n        data_frame_for_duration_woman.loc[len(data_frame_for_duration_woman)] = new_record\n    else:\n        data_frame_for_duration_man.loc[len(data_frame_for_duration_man)] = new_record\n\n    # Wyświetlam postęp pętli.\n    print(ID)\n    clear_output(wait=True)\n\n# Sortuję ramki danych według długości nagrań w kolejności malejącej.\ndata_frame_for_duration_woman = data_frame_for_duration_woman.sort_values(by='duration', ascending=False)\ndata_frame_for_duration_man = data_frame_for_duration_man.sort_values(by='duration', ascending=False)\n\ndata_frame_for_duration_woman.to_csv('data_frame_for_duration_woman.csv', index=False)\ndata_frame_for_duration_man.to_csv('data_frame_for_duration_man.csv', index=False)\n\n# Wybieram 50 kobiet i 50 mężczyzn o najdłuższej sumarycznej długości nagrań.\ntop_50_man = data_frame_for_duration_man.head(50)\ntop_50_woman = data_frame_for_duration_woman.head(50)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"folders_path = '/kaggle/input/audio-to-train-model/train-clean-100'\n\ntop_50_man_paths = [folders_path + '/' + ID for ID in top_50_man['ID']]\ntop_50_woman_paths = [folders_path + '/' + ID for ID in top_50_woman['ID']]\n\ntop_50_man_and_woman = top_50_man_paths + top_50_woman_paths\n\nwith open(\"top_50_man_and_woman.pkl\", \"wb\") as file:\n    pickle.dump(top_50_man_and_woman, file)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_audio_to_slices(path_to_files, seconds):\n    \n    # Przechodzę do katalogów wewnątrz folderu osoby (ID osoby).\n    # Każdy folder wewnętrzny zawiera więcej podfolderów, które mogą zawierać nagrania.\n    paths_inside_ID = [f.name for f in os.scandir(path_to_files) if f.is_dir()]\n\n    # Tworzę pełne ścieżki do podfolderów, aby przejść do wszystkich plików nagrań dla danej osoby.\n    full_paths_to_files = [path_to_files + '/' + path_inside_ID for path_inside_ID in paths_inside_ID]\n\n    # Zbieram wszystkie ścieżki do plików audio danej osoby.\n    # Każdy plik powinien mieć rozszerzenie `.flac`, a wszystkie pliki są przechowywane w zmiennej `all_files_for_ID`.\n\n    all_files_for_ID = []\n    \n    for full_path_to_files in full_paths_to_files:\n        files = [f.name for f in os.scandir(full_path_to_files) if f.is_file() and f.name.endswith('.flac')]\n        files = [full_path_to_files + '/' + file for file in files]\n        all_files_for_ID = all_files_for_ID + files\n\n    # Łączę wszystkie nagrania danej osoby w jedno bardzo długie nagranie.\n    # Używam częstotliwości próbkowania 16kHz (standardowe dla nagrań mowy).\n    sr = 16000\n    combined_signals = np.array([])\n\n    for file_for_ID in all_files_for_ID:\n        signal, sr = librosa.load(file_for_ID, sr=sr)\n        combined_signals = np.concatenate([combined_signals, signal])\n\n\n\n    # Długie nagranie dzielę na  fragmenty o podanej długości.\n    # Fragmenty, które mają mniej niż zadeklarowane długości nagrania (resztki na końcu nagrania), są pomijane.\n    list_for_parts = []\n    len_of_combined_signals = len(combined_signals)\n    step = seconds * sr  # Ustawienie skoku na 5 sekund\n    \n    for i in np.arange(start=0, stop=len_of_combined_signals-step, step=step):\n        list_for_parts.append(combined_signals[i:i+step].tolist())\n\n    parts = np.array(list_for_parts)\n\n    \n\n    # Liczba współczynników MFCC, które zostaną wyliczone dla każdego fragmentu nagrania (standardowe 13 współczynników).\n    quantity_of_mel_coef = 13\n\n    # Liczba filtrów melowych, które określają, ile \"czapek\" melowych zostanie użytych do przetwarzania sygnału.\n    quantity_of_mel_filters = 26\n\n\n    # Liczę MFCC dla którkich fragmentów nagrań\n    mfcc_list = []\n    \n    for i in range(len(parts)):\n        mfcc = librosa.feature.mfcc(y=parts[i], \n                                    sr=16000, \n                                    n_mfcc=quantity_of_mel_coef, \n                                    n_mels=quantity_of_mel_filters).T\n        mfcc_list.append(mfcc)\n\n    mfcc = np.array(mfcc_list)\n\n    # Funkcja zwraca MFCC dla nagrań o długości jednej sekundy\n    return mfcc","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_mfcc_list = []\ntrain_owner = []\n\n# W pętli korzystam z wcześniej zdefiniowanej funkcji, która dzieli nagrania każdej z 100 osób\n# (50 mężczyzn i 50 kobiet) na zestaw treningowy i testowy.\n\nfor i in range(0, 100):\n    \n    # Wywołuję funkcję split_audio_to_slices, aby podzielić nagrania danej osoby na zbiory treningowe i testowe.\n    mfcc = split_audio_to_slices(top_50_man_and_woman[i], seconds = 1)\n\n    # Dodaję uzyskane dane MFCC do odpowiednich list.\n    train_mfcc_list.extend(mfcc)\n\n    # Do listy train_owner i test_owner dodaję identyfikatory osób (i) tyle razy, ile jest nagrań.\n    train_owner.extend([i] * len(mfcc))\n\n    # Wyświetlam numer osoby w pętli, aby śledzić postęp.\n    print(i)\n    clear_output(wait=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Liczę skaler aby sieć dobrze się wytrenowała\n\ncommmon_df_for_train = np.concatenate(train_mfcc_list, axis=0)\n\nscaler = StandardScaler()\n\nscaler.fit(commmon_df_for_train)\n\nX_train = [scaler.transform(one_audio) for one_audio in train_mfcc_list]\n\nX_train = np.array(X_train)\ntrain_owner = np.array(train_owner)\n\nwith open(\"scaler.pkl\", \"wb\") as file:\n    pickle.dump(scaler, file)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Wydzielam zbiór treningowy i walidacyjny do modelu\ntrain_size = int(np.floor(len(X_train) * 0.8))\n\n# Losowy wybór indeksów, które zostaną użyte jako dane treningowe (80% próbek).\nindex_of_train = np.random.choice(np.arange(0, len(X_train)), size=train_size, replace=False)\n\n# Reszta indeksów (20% próbek będzie zbiorem walidacyjnym)\nrest_of_index = ~np.isin(np.arange(0, len(X_train)), index_of_train)\n\nX_valid = X_train[rest_of_index]\nX_train = X_train[index_of_train]\n\nvalid_owner = train_owner[rest_of_index]\ntrain_owner = train_owner[index_of_train]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Przekształcam dane treningowe (X_train), aby miały odpowiedni kształt dla sieci CNN.\n# Dodaję nowy wymiar (1) na końcu, ponieważ sieci konwolucyjne oczekują wejść o formacie 4D:\n# (liczba próbek, wysokość, szerokość, liczba kanałów). Tutaj mamy 1 kanał (monofoniczne nagrania).\n\nX_train = X_train.reshape((X_train.shape[0], \n                           X_train.shape[1], \n                           X_train.shape[2], \n                           1))\n\n# Przekształcam dane walidacyjne (X_valid) do tego samego formatu 4D co dane treningowe.\nX_valid = X_valid.reshape((X_valid.shape[0], \n                           X_valid.shape[1], \n                           X_valid.shape[2], \n                           1))\n\n\n\n# Konwertuję etykiety dla zestawu treningowego (train_owner) na postać one-hot encoding dla klasyfikacji wieloklasowej (100 klas).\n# Używam funkcji to_categorical, aby zamienić numeryczne etykiety (ID osób) na macierze o rozmiarze [100], gdzie \n# każda wartość reprezentuje prawdopodobieństwo przynależności do danej klasy.\ny_train = to_categorical(train_owner, num_classes=100)\n\n# Podobnie konwertuję etykiety dla zestawu walidacyjnego na one-hot encoding.\ny_valid = to_categorical(valid_owner, num_classes=100)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('X_train.npy', X_train)\nnp.save('X_valid.npy', X_valid)\n\nnp.save('y_train.npy', y_train)\nnp.save('y_valid.npy', y_valid)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = np.load('/kaggle/input/ubm-train-data/X_train.npy')\nX_valid = np.load('/kaggle/input/ubm-train-data/X_valid.npy')\n\ny_train = np.load('/kaggle/input/ubm-train-data/y_train.npy')\ny_valid = np.load('/kaggle/input/ubm-train-data/y_valid.npy')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_labels = np.argmax(y_train, axis=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ustawienie ziarna losowości dla powtarzalnych wyników\nnp.random.seed(11)\ntf.random.set_seed(11)\nnp.random.seed(11)\n\n# Inicjalizacja modelu\nmodel = models.Sequential()\n\n# Dodanie warstwy wejściowej z kształtem zgodnym z danymi MFCC \nmodel.add(layers.Input(shape=(32, 13, 1)))\n\n# Pierwsza warstwa konwolucyjna, która ma 64 filtry, okno o rozmiarze 3x3, ReLU jako funkcję aktywacji oraz padding 'same'\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n# Normalizacja wsadowa dla lepszego uczenia\nmodel.add(layers.BatchNormalization())\n# Max Pooling dla zmniejszenia rozmiaru przestrzennego o współczynnik 2\nmodel.add(layers.MaxPooling2D((2, 2)))\n# Dropout dla zapobiegania przeuczeniu (wyłącza 10% neuronów losowo)\nmodel.add(layers.Dropout(0.1))\n\n# Druga warstwa konwolucyjna, zwiększamy liczbę filtrów do 128\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.1))\n\n# Trzecia warstwa konwolucyjna, zwiększamy liczbę filtrów do 256\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.1))\n\n# Czwarta warstwa konwolucyjna, zwiększamy liczbę filtrów do 512\nmodel.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\nmodel.add(layers.BatchNormalization())\n# Dropout dla zapobiegania przeuczeniu (tym razem wyłącza 20% neuronów)\nmodel.add(layers.Dropout(0.2))\n\n# Spłaszczenie wyniku do wektora jednowymiarowego przed przekazaniem go do warstw w pełni połączonych\nmodel.add(layers.Flatten())\n\n# Pierwsza warstwa w pełni połączona (Dense) z 2048 jednostkami, funkcja aktywacji ReLU\nmodel.add(layers.Dense(1024, activation='relu'))\nmodel.add(layers.BatchNormalization())\n# Dropout dla zapobiegania przeuczeniu (wyłącza 75% neuronów)\nmodel.add(layers.Dropout(0.75))\n\n# Warstwa bottleneck\nmodel.add(layers.Dense(64, activation='linear'))\n\n\n\n\n\n# Warstwa wyjściowa - klasyfikacja wieloklasowa z 100 jednostkami i aktywacją softmax (klasyfikacja wieloklasowa)\nmodel.add(layers.Dense(100, activation='softmax'))  # 100 klas (każda klasa reprezentuje innego mówcę)\n\n# Kompilacja modelu - optymalizator Adam, funkcja straty categorical_crossentropy (wieloklasowa klasyfikacja), oraz miara skuteczności - accuracy\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Wyświetlenie podsumowania modelu\nmodel.summary()\n\n# Callbacks: redukcja współczynnika uczenia (learning rate), gdy val_loss przestaje się poprawiać\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8)\n# Wczesne zatrzymanie treningu, jeśli val_loss nie poprawia się przez 10 epok, przywracając najlepsze wagi\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\n# Trenowanie modelu\nhistory = model.fit(\n    X_train,    # Dane MFCC\n    y_train,    # Etykiety one-hot encoded (macierz klas mówców)\n    epochs=500,  # Liczba epok treningu\n    batch_size=32, # Rozmiar batcha (z dotychczasowych testów wynika, że 32 jest optymalne)\n    verbose = 1,\n    validation_data=(X_valid, y_valid),  # Walidacja na wcześniej zdefiniowanych danych walidacyjnych\n    callbacks=[reduce_lr, early_stopping]  # Callbacki do dynamicznej regulacji uczenia i wczesnego zatrzymania\n)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"model.h5\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Funkcja służy do stworzenia embeddingu nagrania, pobiera po prostu wartości jakie są w wartowie bottlneck podczas dokonywania klasyfikacji danego nagrania\ndef calcuate_embedding(one_audio, model):\n    \n    intermediate_layer_model = Model(inputs=model.inputs,\n                                 outputs=model.get_layer('dense_1').output)\n    intermediate_output = intermediate_layer_model.predict(one_audio[np.newaxis, ...])\n    \n    return intermediate_output","metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Liczymy embeddingi nagrań treningowych UBM aby stworzyć LDA na ich podstawie\nX_train_embedding = []\n\n# Iteracja po każdym elemencie w X_train\nfor i in range(0, len(X_train)):\n    # Obliczenie embeddingu dla danego nagrania w X_train przy użyciu modelu\n    train_embedding = calcuate_embedding(X_train[i], model)\n\n    # Dodanie embeddingu do listy X_train_embedding\n    X_train_embedding.append(train_embedding)\n\n    # Wyczyszczenie poprzedniego outputu i wyświetlenie postępu przetwarzania\n    clear_output(wait=True)\n    print(i/len(X_train))  # Wyświetla proporcjonalny postęp jako wartość od 0 do 1\n\n# Zapisanie listy embeddingów X_train_embedding do pliku w formacie pickle\nwith open(\"X_train_embedding.pkl\", \"wb\") as file:\n    pickle.dump(X_train_embedding, file)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Wczytanie embeddingów treningowych z pliku pickle\nwith open(\"C:/Users/zbugo/Desktop/praktyki_zadania/18/good_data/X_train_embedding.pkl\", 'rb') as file:\n    X_train_embedding = pickle.load(file)\n\n# Wczytanie etykiet treningowych z pliku .npy\ny_train = np.load(\"C:/Users/zbugo/Desktop/praktyki_zadania/18/good_data/y_train.npy\")\n# Zamiana etykiet one-hot encoded na numery klas\ny_train_classes = np.argmax(y_train, axis=1)\n\n# Przekształcenie listy embeddingów w macierz NumPy, gdzie każdy wiersz to embedding\nX_train_embedding = np.vstack(X_train_embedding)\n\n# Skalowanie danych do rozkładu o średniej 0 i odchyleniu standardowym 1\nscaler = StandardScaler()\nscaler.fit(X_train_embedding)\nX_train_embedding = scaler.transform(X_train_embedding)\n\n# Dopasowanie LDA (Linear Discriminant Analysis) na przeskalowanych embeddingach\nlda = LDA()\nlda.fit(X=X_train_embedding, y=y_train_classes);","metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":"with open('scaler_after_embedding.pkl', 'wb') as file:\n    pickle.dump(scaler, file)\n\nwith open('lda.pkl', 'wb') as file:\n    pickle.dump(lda, file)","metadata":{},"outputs":[],"execution_count":9}]}